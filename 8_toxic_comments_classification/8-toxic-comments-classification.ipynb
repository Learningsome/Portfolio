{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обзор-данных\" data-toc-modified-id=\"Обзор-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Обзор данных</a></span></li><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Подготовка данных</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Не-BERT-модели\" data-toc-modified-id=\"Не-BERT-модели-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Не BERT модели</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>BERT</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация комментариев c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучим модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Построим модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "---\n",
    "\n",
    "**Шаги выполнения проекта**\n",
    "\n",
    "1. Загрузим и подготовим данные.\n",
    "2. Обучим разные модели. \n",
    "3. Сделайем выводы.\n",
    "\n",
    "---\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`.\n",
    "\n",
    "\n",
    "Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорты из стандартной библиотеки\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# импорты сторонних библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# импорты модулей текущего проекта\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TextClassificationPipeline,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    BertConfig,\n",
    "    BertModel\n",
    ")\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, \n",
    "    RidgeClassifier,\n",
    "    SGDClassifier\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm, notebook\n",
    "\n",
    "# настройки\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas(desc='progress')\n",
    "\n",
    "# константы заглавными буквами\n",
    "RANDOM_STATE = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем файл toxic_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toxic_comments = pd.read_csv('toxic_comments.csv')\n",
    "except:\n",
    "    toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>159446</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>159447</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>159448</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>159449</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>159450</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "159287      159446  \":::::And for the second time of asking, when ...      0\n",
       "159288      159447  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159289      159448  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159290      159449  And it looks like it was actually you who put ...      0\n",
       "159291      159450  \"\\nAnd ... I really don't think you understand...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сразу дропнем ненужный столбец\n",
    "toxic_comments = toxic_comments.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159292</td>\n",
       "      <td>159292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>159292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\"\\n\\n Consonant and Vowel inventory?? \\n\\nHey ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.302139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text          toxic\n",
       "count                                              159292  159292.000000\n",
       "unique                                             159292            NaN\n",
       "top     \"\\n\\n Consonant and Vowel inventory?? \\n\\nHey ...            NaN\n",
       "freq                                                    1            NaN\n",
       "mean                                                  NaN       0.101612\n",
       "std                                                   NaN       0.302139\n",
       "min                                                   NaN       0.000000\n",
       "25%                                                   NaN       0.000000\n",
       "50%                                                   NaN       0.000000\n",
       "75%                                                   NaN       0.000000\n",
       "max                                                   NaN       1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# есть дисбаланс\n",
    "toxic_comments['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме дисбаланса проблем не вижу, все нормально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать spacy для лемматизации (pymystem работал очень долго и я от него отказался).\n",
    "\n",
    "Объявим функции lemmatize() и clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):  \n",
    "    # лемматизируем текст используя модуль nlp\n",
    "    doc = nlp(text)\n",
    "    lemm_text = ' '.join([token.lemma_ for token in doc]).lower()\n",
    "    \n",
    "    return lemm_text\n",
    "\n",
    "\n",
    "def clear_text(lemm_text):\n",
    "    # чистим текст\n",
    "    cleared_text = re.sub(r'[^a-zA-Z\\']', ' ', lemm_text).split()\n",
    "    cleared_text = ' '.join(cleared_text)\n",
    "    \n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем скачать файл с лемматизированными данными, если его нет, лемматизируем текст и сохраняем файлик.\n",
    "\n",
    "\n",
    "Также распараллелим процесс, чтобы лемматизация прошла быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# грузим модуль для обработки текста\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим progress_apply\n",
    "try:\n",
    "    toxic_comments = pd.read_csv('toxic_lemm.csv', index_col=[0])\n",
    "except:\n",
    "    toxic_comments['lemm_text'] = toxic_comments['text'].progress_apply(lambda x: clear_text(lemmatize(x)))\n",
    "    toxic_comments.to_csv('toxic_lemm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод**:\n",
    "\n",
    "- данные загружены и изучены\n",
    "- дропнут лишний столбец\n",
    "- удалены строки с кириллицей\n",
    "- объявили функции lemmatize и clear text\n",
    "- лемматизировали и очистили текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датасет на train и test выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          0\n",
       "toxic         0\n",
       "lemm_text    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# избавимся от 10 пропусков\n",
    "toxic_comments = toxic_comments[~toxic_comments['lemm_text'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обозначим признак и таргет\n",
    "X = toxic_comments['lemm_text']\n",
    "y = toxic_comments['toxic']\n",
    "\n",
    "# сохраним отношение классов stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898323\n",
       "1    0.101677\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ОК\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nikolaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# качаем стоп слова\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# содаем лист стоп слов на английском\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер трейн матрицы: (143120, 147217)\n",
      "Размер тест матрицы: (15903, 147217)\n"
     ]
    }
   ],
   "source": [
    "# учитывая стоп-слова считаем TF-IDF для трейн и тест корпуса\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "tf_idf_train = count_tf_idf.fit_transform(X_train)\n",
    "tf_idf_test = count_tf_idf.transform(X_test)\n",
    "\n",
    "# выведем размеры матриц\n",
    "print(\"Размер трейн матрицы:\", tf_idf_train.shape)\n",
    "print(\"Размер тест матрицы:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не BERT модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем получить cv_score без использования пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TF-IDF F1 CV SCORE: 0.75\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# создаем эстиматор\n",
    "lr_model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# считаем cv f1-score\n",
    "lr_cv_score = cross_val_score(lr_model, tf_idf_train, y_train, scoring='f1', cv=5).sum() / 5\n",
    "\n",
    "print(f'LR TF-IDF F1 CV SCORE: {lr_cv_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем с применением пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TF-IDF F1 CV SCORE: 0.75\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# пишем пайплайн для логистической регрессии\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=stop_words)),\n",
    "        (\"clf\", LogisticRegression(class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# считаем cv f1-score\n",
    "lr_cv_score = cross_val_score(lr_pipeline, X_train, y_train, scoring='f1', cv=5).sum() / 5\n",
    "print(f'LR TF-IDF F1 CV SCORE: {lr_cv_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существенной разницы нет, но, мы избавились от утечки данных, это хорошо.\n",
    "\n",
    "\n",
    "Теперь используем пайплайн с gridsearch, попробуем подобрать гиперпараметр C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR GRID BEST PARAMS: {'clf__C': 6}\n",
      "LR GRID BEST SCORE: 0.77\n",
      "---------------------------\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_grid_params = {'clf__C': range(5, 16, 1)}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    lr_pipeline, param_grid=lr_grid_params, \n",
    "    scoring='f1', n_jobs=-1, cv=5\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'LR GRID BEST PARAMS: {lr_grid.best_params_}')\n",
    "print(f'LR GRID BEST SCORE: {lr_grid.best_score_:.2f}')\n",
    "print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовал также и модели RidgeClassifier и SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры: {'clf__tol': 0.001, 'clf__max_iter': 1000, 'clf__alpha': 1, 'clf': RidgeClassifier(alpha=1, class_weight='balanced', max_iter=1000,\n",
      "                random_state=12345, tol=0.001)}\n",
      "Оценка качества: 0.6982359588723536\n"
     ]
    }
   ],
   "source": [
    "# создание конвейера для обработки текстовых данных и классификации\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=stop_words)),\n",
    "        (\"clf\", RidgeClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# задание сетки гиперпараметров для Ridge и SGDC\n",
    "parameters = {\n",
    "    \"clf\": [\n",
    "        RidgeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        SGDClassifier(\n",
    "            loss=\"log_loss\", class_weight=\"balanced\", random_state=RANDOM_STATE\n",
    "        ),\n",
    "    ],\n",
    "    \"clf__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"clf__max_iter\": [1000, 5000, 10000],\n",
    "    \"clf__tol\": [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "# создание объекта GridSearchCV для подбора гиперпараметров\n",
    "randomized_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# запуск поиска по сетке\n",
    "randomized_search.fit(X_train, y_train)\n",
    "\n",
    "# вывод лучших гиперпараметров и оценки качества\n",
    "print(f\"Лучшие гиперпараметры: {randomized_search.best_params_}\")\n",
    "print(f\"Оценка качества: {randomized_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка вышла не лучшая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем использовать пайплайн и для lgbm классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM TF-IDF F1 CV SCORE: 0.74\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# объявляем пайплайн для lgbm\n",
    "lgbm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=stop_words)),\n",
    "        (\"clf\", LGBMClassifier(\n",
    "            class_weight='balanced', n_jobs=-1, \n",
    "            random_state=RANDOM_STATE)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# считаем cv f1-score\n",
    "lgbm_cv_score = cross_val_score(lgbm_pipeline, X_train, y_train, scoring='f1', cv=5, n_jobs=-1).sum() / 5\n",
    "print(f'LGBM TF-IDF F1 CV SCORE: {lgbm_cv_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить обычную модель BERT без тюнинга, для начала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9ff064a4fe4cc690b1e1edcf81ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a1429700e44ab4b3e515440c88dfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e4e63c961f4b859165e0758ac582de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f107a3364f4742a3aaed55d5142d2180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671a550a7ef9464cb1886e2fb2fee8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:02<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT F1-SCORE ON SAMPLE: 0.96\n",
      "Wall time: 3min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# задаем путь претренированной модели и device \n",
    "model_path = 'unitary/toxic-bert'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# создаем токенайзер + модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "# создаем семпл из 1000 строк\n",
    "test_bert = toxic_comments.sample(1000, weights='toxic', random_state=RANDOM_STATE)\n",
    "\n",
    "# предиктим токсичные комментарии\n",
    "y_pred = []\n",
    "for text in tqdm(test_bert['text']):\n",
    "    tokenized = tokenizer.encode( text, truncation=True, max_length=512, add_special_tokens=True)\n",
    "    tokenized = np.array(tokenized)\n",
    "    attention_mask = np.where(tokenized != 0, 1, 0)\n",
    "    batch = torch.tensor([tokenized]).to(device) \n",
    "    attention_mask_batch = torch.tensor([attention_mask]).to(device)\n",
    "\n",
    "    pred = model(batch, attention_mask_batch)\n",
    "\n",
    "    y_pred.append(1 if bool(pred[0][0][0].cpu().detach() > 0) else 0)\n",
    "    \n",
    "# считаем скор и выводим на экран\n",
    "bert_f1_score = f1_score(test_bert['toxic'], y_pred)\n",
    "print(f'BERT F1-SCORE ON SAMPLE: {bert_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На сэмпле из 10000 значений BERT показал себя отлично, на полном наборе данных, думаю, можно ожидать похожей оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрый тест LogisticRegression на test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR BEST MODEL F1-SCORE: 0.763\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = lr_grid.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "best_model_pred = best_model.predict(X_test)\n",
    "best_model_f1_score = f1_score(y_test, best_model_pred)\n",
    "\n",
    "print(f'LR BEST MODEL F1-SCORE: {best_model_f1_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод**:\n",
    "\n",
    "- избавились от пропусков после лемматизации\n",
    "- разбили датасет на train и test, учли дисбаланс\n",
    "- учитывая стоп-слова посчитали TF-IDF для train и test корпуса\n",
    "- обучили *LogisticRegression* (F1-score: 0.77) и *LGBMClassifier* (F1-score: 0.74)\n",
    "- *BERT* на семпле из 1000 строк показал себя **отлично**, F1-score: 0.96\n",
    "---\n",
    "- на тесте LogisticRegression c подбором гиперпараметра C показала F1-score: 0.763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Общий вывод`**:\n",
    "\n",
    "\n",
    "- **Подготовка**:\n",
    "\n",
    "    - данные загружены и изучены\n",
    "    - дропнут лишний столбец\n",
    "    - удалены строки с кириллицей\n",
    "    - объявили функции lemmatize и clear text\n",
    "    - лемматизировали и очистили текст\n",
    "\n",
    "\n",
    "- **Обучение**:\n",
    "\n",
    "    - избавились от пропусков после лемматизации\n",
    "    - разбили датасет на train и test, учли дисбаланс\n",
    "    - учитывая стоп-слова посчитали TF-IDF для train и test корпуса\n",
    "    - обучили *LogisticRegression* (F1-score: 0.77) и *LGBMClassifier* (F1-score: 0.74)\n",
    "    - *BERT* на семпле из 1000 строк показал себя **отлично**, F1-score: 0.76\n",
    "---\n",
    "   - на тесте LogisticRegression c подбором гиперпараметра C показала F1-score: 0.763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361.383px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
